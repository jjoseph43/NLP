{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8332211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "886b39b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows-10-10.0.22000-SP0\n",
      "Python\n",
      "nltk\n",
      "requests\n",
      "re\n"
     ]
    }
   ],
   "source": [
    "import platform; print( platform.platform())\n",
    "import sys; print (\"Python\"), sys.version\n",
    "import nltk; print (\"nltk\"), nltk.__version__\n",
    "#from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests; print (\"requests\"), requests.__version__\n",
    "\n",
    "try :\n",
    "    from urllib2 import Request, urlopen\n",
    "except :\n",
    "    from urllib.request import Request, urlopen\n",
    "    \n",
    "import re; print (\"re\"), re.__version__\n",
    "\n",
    "from pattern.en import parsetree\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa36160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pattern\n",
    "\n",
    "import import_ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b996e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from html.parser import HTMLParser\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5f3707c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-0cc12fc33b9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcontractions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCONTRACTION_MAP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from HTMLParser import HTMLParser\n",
    "import unicodedata\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "html_parser = HTMLParser()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "    \n",
    "    \n",
    "from pattern.en import tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    tagged_text = tag(text)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text\n",
    "    \n",
    "# lemmatize text based on POS tags    \n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "    \n",
    "\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "    \n",
    "    \n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def unescape_html(parser, text):\n",
    "    \n",
    "    return parser.unescape(text)\n",
    "\n",
    "def normalize_corpus(corpus, lemmatize=True, tokenize=False):\n",
    "    \n",
    "    normalized_corpus = []  \n",
    "    for text in corpus:\n",
    "        text = html_parser.unescape(text)\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        if lemmatize:\n",
    "            text = lemmatize_text(text)\n",
    "        else:\n",
    "            text = text.lower()\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "        else:\n",
    "            normalized_corpus.append(text)\n",
    "            \n",
    "    return normalized_corpus\n",
    "\n",
    "\n",
    "def parse_document(document):\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    if isinstance(document, str):\n",
    "        document = document\n",
    "    elif isinstance(document, unicode):\n",
    "        return unicodedata.normalize('NFKD', document).encode('ascii', 'ignore')\n",
    "    else:\n",
    "        raise ValueError('Document is not string or unicode!')\n",
    "    document = document.strip()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    return sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7160a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6298a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def build_feature_matrix(documents, feature_type = 'frequency',\n",
    "                         ngram_range = (1, 1), min_df = 0.0, max_df = 1.0):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "    \n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary = True, min_df = min_df,\n",
    "                                     max_df = max_df, ngram_range = ngram_range)\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary = False, min_df = min_df,\n",
    "                                     max_df = max_df, ngram_range = ngram_range)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df = min_df, max_df = max_df, \n",
    "                                     ngram_range = ngram_range)\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    \n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb6bd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... ref : Text Analytics with Python, Sarker, p. 287\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "def compute_cosine_similarity(doc_features, corpus_features, top_n = 3):\n",
    "\n",
    "    # get document vectors\n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "\n",
    "    # compute similarities\n",
    "    similarity = np.dot(doc_features, \n",
    "                        corpus_features.T)\n",
    "\n",
    "    # get docs with highest similarity scores\n",
    "    top_docs = similarity.argsort()[::-1][:top_n]\n",
    "    \n",
    "    top_docs_with_score = [(index, round(similarity[index], 3))\n",
    "                            for index in top_docs]\n",
    "    \n",
    "    return top_docs_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b576bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... list of book titles from Amazon\n",
    "# ... search key word entered : \"Underworld\"\n",
    "# ... list below represents the 1st 24 titles returned from Amazon search\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "corpus = [\"Underworld: A Novel\",\n",
    "\"Underworld - Through the Belly of the Beast: A LitRPG Series\",\n",
    "\"Underworld: The Mysterious Origins of Civilization\",\n",
    "\"Lords of the Underworld Collection Volume 1: An Anthology\",\n",
    "\"The Darkest Warrior (Lords of the Underworld Book 14)\",\n",
    "\"The Underworld U.S.A. Trilogy, Volume I: American Tabloid, The Cold Six Thousand (Everyman's Library Contemporary Classics Series)\",\n",
    "\"Lords of the Underworld Collection Volume 2: An Anthology\",\n",
    "\"The Social Order of the Underworld: How Prison Gangs Govern the American Penal System\",\n",
    "\"Lords of the Underworld: The Darkest Sampler\",\n",
    "\"City of Devils: The Two Men Who Ruled the Underworld of Old Shanghai\",\n",
    "\"The Corporation: An Epic Story of the Cuban American Underworld\",\n",
    "\"The Dark Net: Inside the Digital Underworld\",\n",
    "\"Kings of the Underworld: Alpha & Omega\",\n",
    "\"The Darkest Night (Lords of the Underworld Book 1)\",\n",
    "\"The Underworld U.S.A. Trilogy, Volume II: Blood's A Rover\",\n",
    "\"SAINT (Boston Underworld Book 4)\",\n",
    "\"REAPER (Boston Underworld Book 2)\",\n",
    "\"Servant of the Underworld (Obsidian and Blood) (Volume 1)\",\n",
    "\"Go to Hell: A Heated History of the Underworld\",\n",
    "\"GHOST (Boston Underworld Book 3)\",\n",
    "\"The Arraignment III: The Underworld\",\n",
    "\"THIEF (Boston Underworld Book 5)\",\n",
    "\"King Tut: The Journey through the Underworld\",\n",
    "\"Underworld - Level Up or Die: A LitRPG Series\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a3f569a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-350970250b87>:5: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  text = html_parser.unescape(text)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'expand_contractions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-f42bd9007f42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# normalize and extract features from the toy corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mnorm_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m#norm_corpus = normalize_corpus(query_docs, lemmatize = True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-350970250b87>\u001b[0m in \u001b[0;36mnormalize_corpus\u001b[1;34m(corpus, lemmatize, tokenize)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munescape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpand_contractions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCONTRACTION_MAP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'expand_contractions' is not defined"
     ]
    }
   ],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... ref : Text Analytics with Python, Sarker, pp. 286, 287\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "query_docs = corpus\n",
    "\n",
    "html_parser = HTMLParser()\n",
    "\n",
    "# normalize and extract features from the toy corpus\n",
    "\n",
    "norm_corpus = normalize_corpus(corpus, lemmatize = True)\n",
    "#norm_corpus = normalize_corpus(query_docs, lemmatize = True)\n",
    "\n",
    "tfidf_vectorizer, tfidf_features = build_feature_matrix(norm_corpus,\n",
    "                                                        feature_type = 'tfidf',\n",
    "                                                        ngram_range = (1, 1), \n",
    "                                                        min_df = 0.0,\n",
    "                                                        max_df = 1.0)\n",
    "                                                        \n",
    "# normalize and extract features from the query corpus\n",
    "\n",
    "norm_query_docs =  normalize_corpus(query_docs, lemmatize = True)    \n",
    "#norm_query_docs =  normalize_corpus(corpus, lemmatize = True)         \n",
    "\n",
    "query_docs_tfidf = tfidf_vectorizer.transform(norm_query_docs)\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... cosine similarity\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "    \n",
    "print ('='*60)\n",
    "print ('Book Titles Similarity Analysis using Cosine Similarity')\n",
    "print ('='*60)\n",
    "print (\"\\n\")\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... return top 2 similar, then drop the 1st if same as primary document\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "for index, doc in enumerate(query_docs):\n",
    "    \n",
    "    doc_tfidf = query_docs_tfidf[index]\n",
    "    \n",
    "    top_similar_docs = compute_cosine_similarity(doc_tfidf,\n",
    "                                             tfidf_features,\n",
    "                                             top_n = 2)\n",
    "    \n",
    "    if(top_similar_docs[0][0] == index) :\n",
    "        tsd = top_similar_docs[1]\n",
    "    else :\n",
    "        tsd = top_similar_docs[0]\n",
    "            \n",
    "    print ('-'*80)\n",
    "    buffer = \"Title %2d         : %s\" % (index, doc)\n",
    "    print (buffer)\n",
    "    print ('Cosine similar   :'), corpus[tsd[0]]\n",
    "    print ('Similarity score :'), tsd[1]\n",
    "    print ('-'*80) \n",
    "    print (\"\\n\")                                            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
