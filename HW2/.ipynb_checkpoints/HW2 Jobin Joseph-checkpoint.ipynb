{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c65ccd5",
   "metadata": {},
   "source": [
    "### 1.\tIn Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857518b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37892261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url = \"https://www.gutenberg.org/cache/epub/14668/pg14668.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c7ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the three books and tokenizing based on words\n",
    "tokenstext1 = word_tokenize(raw)\n",
    "type(tokenstext1)\n",
    "len(tokenstext1)\n",
    "textsecond = nltk.Text(tokenstext1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7beffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/15040/pg15040.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "type(raw)\n",
    "tokenstext2 = word_tokenize(raw)\n",
    "type(tokenstext2)\n",
    "len(tokenstext2)\n",
    "textfifth = nltk.Text(tokenstext2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b88bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/16751/pg16751.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "type(raw)\n",
    "tokenstext3 = word_tokenize(raw)\n",
    "type(tokenstext3)\n",
    "len(tokenstext3)\n",
    "textsixth = nltk.Text(tokenstext3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1e34b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "all_words = words.words()\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6e00e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding vocabulary size of the texts\n",
    "#making a vocab method\n",
    "def vocabsize(inputtext):\n",
    "    return len(set(inputtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2853d948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size  of first text is  4022\n",
      "Vocab size  of fifth text is  14290\n",
      "Vocab size  of sixth text is  17259\n"
     ]
    }
   ],
   "source": [
    "firsttext = vocabsize(textsecond)\n",
    "print(\"Vocab size  of first text is \", firsttext)\n",
    "\n",
    "fifthtext = vocabsize(textfifth)\n",
    "print(\"Vocab size  of fifth text is \", fifthtext)\n",
    "\n",
    "sixthtext = vocabsize(textsixth)\n",
    "print(\"Vocab size  of sixth text is \", sixthtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851b377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing between zero and one   using  the total corpus\n",
    "#method to compare between the  size of the words ans the total words\n",
    "def vocabnorm(text):\n",
    "    return (vocabsize(text))/len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b94ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size  of first text normalized is  0.017\n",
      "Vocab size  of fifth text normalized is  0.060\n",
      "Vocab size  of sixth text normalized is  0.073\n"
     ]
    }
   ],
   "source": [
    "firsttext = vocabnorm(textsecond)\n",
    "firsttextnew = \"{:.3f}\".format(firsttext)\n",
    "print(\"Vocab size  of first text normalized is \", firsttextnew)\n",
    "\n",
    "fifthtext = vocabnorm(textfifth)\n",
    "fifthtextnew = \"{:.3f}\".format(fifthtext)\n",
    "print(\"Vocab size  of fifth text normalized is \", fifthtextnew)\n",
    "\n",
    "sixthtext = vocabnorm(textsixth)\n",
    "sixthtexttnew = \"{:.3f}\".format(sixthtext)\n",
    "print(\"Vocab size  of sixth text normalized is \", sixthtexttnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e681d3",
   "metadata": {},
   "source": [
    " These  three texts will be normalized using the the unique words of  the text against the total corpa which has all the words. There should not be a text that has all the word in it.  This allows the scale of the values be within 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278c22c",
   "metadata": {},
   "source": [
    "## 2.\tAfter consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8baab577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fromtextbook \n",
    "#V = set(text1)\n",
    "#long_words = [w for w in V if len(w) > 11]\n",
    "def longwordnorm (text, L):\n",
    "    longwordif=[w for w in set(text) if len(w) > L] \n",
    "    number_of_Longwords = len(longwordif)\n",
    "    longwordnormalized = (number_of_Longwords/len(all_words))\n",
    "    return longwordnormalized \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a9d949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003700324412003244\n"
     ]
    }
   ],
   "source": [
    "text1longnorm = longwordnorm(textsecond, 7)\n",
    "print(text1longnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d0181e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022624357934576914\n"
     ]
    }
   ],
   "source": [
    "text2longnorm = longwordnorm(textfifth, 7)\n",
    "print(text2longnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0bb5139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029133718572587185\n"
     ]
    }
   ],
   "source": [
    "text3longnorm = longwordnorm(textsixth, 7)\n",
    "print(text3longnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c76eefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# did was not used as there a method was made later ^^^^\n",
    "\n",
    "# Finding the longword over a certain lenght for 1st book\n",
    "#newtext = longword(textsecond, 7)\n",
    "#print (newtext)\n",
    "#number_of_Longwords = len(newtext)\n",
    "\n",
    "#print(\"Number of longwords in the text \", number_of_Longwords)\n",
    "\n",
    "#normalizing  using the same metric as above\n",
    "#longwordnormalized = (number_of_Longwords/len(all_words))\n",
    "#print(\"Number of longwords in the text normalized \", longwordnormalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619d024",
   "metadata": {},
   "source": [
    " These  three texts will be normalized using the the number of word that greater than 7 characters of  the text against the total corpa which has all the words. Each text had different amounts of long word that were included.  This allows the scale of the values be within 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a0ab4",
   "metadata": {},
   "source": [
    "### 3.\tNow create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5366b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5afe7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016989389024060556\n",
      "0.06036259799945931\n",
      "0.07290399432278995\n"
     ]
    }
   ],
   "source": [
    "print (lexical_diversity(textsecond))\n",
    "print (lexical_diversity(textfifth))\n",
    "print (lexical_diversity(textsixth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5885a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textdifficultyscore(text,k):\n",
    "    textdifficulty = lexical_diversity(text) + vocabnorm(text) +longwordnorm(text,k)\n",
    "    return print (textdifficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23c419c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03457015409570154\n"
     ]
    }
   ],
   "source": [
    "textdifficultyscore(textsecond, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f23c8104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1255787037037037\n"
     ]
    }
   ],
   "source": [
    "textdifficultyscore(textfifth, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ac13252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15211459178156259\n"
     ]
    }
   ],
   "source": [
    "textdifficultyscore(textsixth, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47710cac",
   "metadata": {},
   "source": [
    "I have employed lexical diverstiy again from the first homework. Then I made a function from adding all three  normalized values. As no surprise  the sixth grade level test shows a higher level  of text difficulty. It is important to knwo since all three input score were all normalized with the same  parameter they can be added with equal weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b687c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
